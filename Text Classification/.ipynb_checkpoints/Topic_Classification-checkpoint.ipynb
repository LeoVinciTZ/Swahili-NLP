{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-05-27T11:15:21.343107Z",
     "iopub.status.busy": "2020-05-27T11:15:21.343107Z",
     "iopub.status.idle": "2020-05-27T11:15:21.348054Z",
     "shell.execute_reply": "2020-05-27T11:15:21.347088Z",
     "shell.execute_reply.started": "2020-05-27T11:15:21.343107Z"
    }
   },
   "source": [
    "## IMPORT LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import fasttext as ft\n",
    "from sklearn.model_selection import train_test_split\n",
    "from slugify import Slugify\n",
    "from pprint import pprint\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Topic_Labels.csv\", dtype=str, low_memory=False, encoding=\"utf-8\")\n",
    "df['text'] = df.text.astype(str)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['topic_Name','topic']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_slugify = Slugify(to_lower=True)\n",
    "df['topic_Name'] = df['topic_Name'].map(custom_slugify)\n",
    "df.groupby(['topic']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declared ordered Topics Names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names_list = ['Health', 'Nutrition', 'Education', 'HIV Aids', 'Violence Against Children (VAC)', 'WASH', 'Mestrual Hygiene', 'Others','Corona','U-Report']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing steps (Comment as required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_for_prediction(list_text):\n",
    "    new_list = []\n",
    "    typos_df = pd.read_csv(\"../Typos.csv\", dtype=str, low_memory=False, encoding=\"utf-8\")\n",
    "    slangs_df = pd.read_csv(\"../Slangs.csv\", dtype=str, low_memory=False, encoding=\"utf-8\")\n",
    "    stopwords_df = pd.read_csv(\"../Stopwords.csv\", dtype=str, low_memory=False, encoding=\"utf-8\")\n",
    "    #stopwords_df['StopWords'] = \" \" + stopwords_df['StopWords'] + \" \"\n",
    "    stopwords_list = list(stopwords_df['StopWords'])\n",
    "    \n",
    "    \n",
    "    for x in list_text:\n",
    "        x = re.sub(r'[^\\w\\s]',' ',x)    # Remove Panctuations /?!.\n",
    "        x = x.strip()                   # Remove leading and trailing spaces\n",
    "        x = re.sub(' +', ' ', x)        # Remove extra white spaces\n",
    "        x = re.sub('[^A-Za-z0-9]+', ' ', x)  # Remove special characters\n",
    "        x = x.lower()                   # Converts to lower case\n",
    "        x = ' '+x+' '                   # Makes sure there is a single white space leading and trailing\n",
    "        for index, row in typos_df.iterrows():\n",
    "            x = x.replace(\" \"+str(row['Typo'])+\" \", \" \"+row['Word']+\" \")         # Replaces typos\n",
    "        for index, row in slangs_df.iterrows():\n",
    "            x = x.replace(\" \"+row['Slang']+\" \", \" \"+row['Meaning']+\" \")          # Replaces slangs\n",
    "        x = ' '.join([word for word in x.split() if word not in stopwords_list]) # Removes stopwords\n",
    "        #print(x)\n",
    "        new_list.append(x)\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [\"hedhi    pedi?virusi! paka sanitaiza/mikono udumavu  /  wa MTOTO'Elim  km 19   ÃƒÂƒÃ‚ÂƒÃƒÂ‚Ã‚Â£ÃƒÂƒÃ‚Â‚ÃƒÂ‚Ã‚Âº xul Kituo ya afya   kinatoa huduma mbaya wajawazito  kwa na mimi \"]\n",
    "docs_new = ['    hedhi    pedi?', 'virusi!','paka sanitaiza/mikono ','udumavu  /  wa MTOTO','Elim  km 19   ÃƒÂƒÃ‚ÂƒÃƒÂ‚Ã‚Â£ÃƒÂƒÃ‚Â‚ÃƒÂ‚Ã‚Âº xul', '   Kituo ya afya   kinatoa huduma mbaya wajawazito  kwa na mimi  ']\n",
    "docs_new = clean_for_prediction(docs_new)\n",
    "print(docs_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Dataset into TRAIN and TEST sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def our_train_test_split(df, test_size=.25):\n",
    "    train = pd.DataFrame(columns=['text','topic'])\n",
    "    test= pd.DataFrame(columns=['text','topic'])\n",
    "    df_list = []\n",
    "    classes = df.topic.unique()\n",
    "    \n",
    "    for c in classes:\n",
    "        df_list.append(df[df.topic==c])\n",
    "        \n",
    "    for dfs in df_list:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(dfs.text, dfs.topic, test_size=test_size, random_state=42, shuffle=True)\n",
    "        train = train.append(pd.concat([pd.DataFrame(X_train), pd.DataFrame(y_train).reindex(pd.DataFrame(X_train).index)], axis=1))\n",
    "        test = test.append(pd.concat([pd.DataFrame(X_test), pd.DataFrame(y_test).reindex(pd.DataFrame(X_test).index)], axis=1))\n",
    "    \n",
    "    return train, test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = our_train_test_split(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Stochastic Gradient Disent Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 SGDClassifier using step-by-step and TF-IDF-Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(ngram_range=(1,1)) \n",
    "\n",
    "count_vect.fit(df['text'])\n",
    "X_train_counts = count_vect.transform(train['text'])\n",
    "\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect.vocabulary_.get(u'elimu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "sgd_clf = SGDClassifier(loss='hinge', penalty='l2',\n",
    "                           alpha=1e-3, random_state=42,\n",
    "                           max_iter=5, tol=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_counts = count_vect.transform(test['text'])\n",
    "print(X_test_counts.shape)\n",
    "\n",
    "X_test_tfidf = tfidf_transformer.transform(X_test_counts)\n",
    "print(X_test_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_clf.fit(X_train_tfidf,  train['topic'])\n",
    "predicted = sgd_clf.predict(X_test_tfidf)\n",
    "sc = np.mean(predicted == test['topic'])\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(test['topic'], predicted, target_names=target_names_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_new = ['hedhi pedi', 'elimu ya afya haiwafikii walengwa','paka sanitaiza mikono ','udumavu wa mtoto','elimu inayotolewa', 'Kituo afya kinatoa huduma mbaya wajawazito','watoto wanatumikishwa kingono']\n",
    "docs_new = clean_for_prediction(docs_new)\n",
    "X_new_counts = count_vect.transform(docs_new)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "predicted = sgd_clf.predict(X_new_tfidf)\n",
    "print(predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 SGDClassifier using PIPELINE and TF-IDF Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "sgd_tfidf_pipeline = Pipeline([\n",
    "     ('vect', CountVectorizer()),\n",
    "     ('tfidf', TfidfTransformer()),\n",
    "     ('clf', SGDClassifier(loss='modified_huber', penalty='l2',\n",
    "                           alpha=1e-3, random_state=42,\n",
    "                           max_iter=5, tol=None)),\n",
    " ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING ALGORITHM- Fit Pipeline\n",
    "sgd_tfidf_model = sgd_tfidf_pipeline.fit(train['text'], train['topic'])\n",
    "sgd_tfidf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = sgd_tfidf_model.score(test['text'], test['topic'] )\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to file in the current working directory\n",
    "joblib_file = \"Models/sgd_tfidf_model.pkl\"\n",
    "joblib.dump(sgd_tfidf_model, joblib_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 SGDClassifier using PIPELINE and TF-Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "sgd_tf_pipeline = Pipeline([\n",
    "     ('vect', CountVectorizer()),\n",
    "     ('tf', TfidfTransformer(use_idf=False)),\n",
    "     ('clf', SGDClassifier(loss='modified_huber', penalty='l2',\n",
    "                           alpha=1e-3, random_state=42,\n",
    "                           max_iter=5, tol=None)),\n",
    " ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING ALGORITHM- Fit Pipeline\n",
    "sgd_tf_model = sgd_tf_pipeline.fit(train['text'], train['topic'])\n",
    "sgd_tf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = sgd_tf_model.score(test['text'], test['topic'] )\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to file in the current working directory\n",
    "joblib_file = \"Models/sgd_tf_model.pkl\"\n",
    "joblib.dump(sgd_tf_model, joblib_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "predicted_test = sgd_tf_model.predict(test['text'])\n",
    "print(metrics.classification_report(test['topic'], predicted_test,target_names=target_names_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text = ['hedhi pedi', 'virusi','paka sanitaiza mikono ','udumavu wa mtoto','elimu inayotolewa', 'Kituo afya kinatoa huduma mbaya wajawazito']\n",
    "new_text_cleaned = clean_for_prediction(new_text)\n",
    "predicted = sgd_tf_model.predict(new_text_cleaned)\n",
    "print(predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Support Vector Machines Classifier(SKLEARN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "svm_pipeline = Pipeline([\n",
    "     ('vect', CountVectorizer(ngram_range=(1,1))),\n",
    "     ('tfidf', TfidfTransformer()),\n",
    "     ('clf', SVC(kernel = 'sigmoid', random_state=0, gamma='scale', C=1.2, probability=True)), #or c=1.3\n",
    " ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model = svm_pipeline.fit(train['text'], train['topic'])\n",
    "svm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = svm_model.score(test['text'], test['topic'])\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to file in the current working directory\n",
    "joblib_file = \"Models/SVM_model.pkl\"\n",
    "joblib.dump(svm_model, joblib_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(svm_pipeline,df['text'], df['topic'], cv=5, scoring='f1_weighted')\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean()*100, scores.std() * 2*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "predicted_test = svm_model.predict(test['text'])\n",
    "print(metrics.classification_report(test['topic'], predicted_test,target_names=target_names_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text = ['hedhi pedi Mwanamke huwa anapata siku zake na damu nyingi hutoka na kumchafua asipotumia pedi wakati wa hedhi']\n",
    "new_text_cleaned = clean_for_prediction(new_text)\n",
    "predicted = svm_model.predict(new_text_cleaned)\n",
    "print(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_pipeline(gamma, C):\n",
    "    svm_pipeline2 = Pipeline([\n",
    "         ('vect', CountVectorizer()),\n",
    "         ('tfidf', TfidfTransformer()),\n",
    "         ('clf', SVC(kernel='rbf', random_state=0, gamma=gamma, C=C, probability=True)),\n",
    "     ])\n",
    "    return svm_pipeline2.fit(train['text'], train['topic'])\n",
    "\n",
    "gamma_range = np.linspace(0.1, 1, 10)\n",
    "C_range = np.arange(10, 110, 10)\n",
    "print(gamma_range)\n",
    "print(C_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = 0\n",
    "c = 0\n",
    "max_score = 0\n",
    "for gamma in gamma_range:\n",
    "    for C in C_range:\n",
    "        svm_model = my_pipeline(gamma, C)\n",
    "        score = svm_model.score(test['text'], test['topic'])\n",
    "        print(\"For gamma = \",gamma, \", C = \", C, \": Score = \", score)\n",
    "        if score > max_score:\n",
    "            max_score = score\n",
    "            c = C\n",
    "            g = gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Gamma: \", g)\n",
    "print(\"C: \", C)\n",
    "print(\"Best Score: \", max_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Search with Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "C = [round(x,1) for x in np.linspace(0.1, 1.0, 10)]\n",
    "gamma = ['scale', 'auto']\n",
    "kernel = ['rbf','poly','linear','sigmoid']\n",
    "class_weight= ['balanced',None]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {\n",
    "               'C': C,\n",
    "               'gamma': gamma,\n",
    "               'kernel': kernel,\n",
    "    'class_weight': class_weight\n",
    "              }\n",
    "\n",
    "pprint(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "sv = SVC(random_state = 0)\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "sv_random = RandomizedSearchCV(estimator=sv, param_distributions=random_grid,\n",
    "                              n_iter = 100, scoring='neg_mean_absolute_error', \n",
    "                              cv = 5, verbose=2, random_state=0, n_jobs=-1,\n",
    "                              return_train_score=True)\n",
    "# Fit the random search model\n",
    "sv_random.fit(X_train_tfidf, train['topic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Function\n",
    "def evaluate(model, test_features, test_labels):\n",
    "    \n",
    "    return model.score(test_features, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the Default Model\n",
    "base_model = SVC()\n",
    "base_model.fit(X_train_tfidf, train['topic'])\n",
    "base_accuracy = evaluate(base_model, X_test_tfidf, test['topic'])\n",
    "base_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the Best Random Search Model\n",
    "best_random = rf_random.best_estimator_\n",
    "random_accuracy = evaluate(best_random, X_test_tfidf, test['topic'])\n",
    "random_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Improvement of {:0.2f}%.'.format( 100 * (random_accuracy - base_accuracy) / base_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "               'C':  [1.0, 1.1, 1.2, 1.3, 1.4, 1.5],\n",
    "               'gamma': ['scale', 'auto'],\n",
    "               'kernel': ['rbf','poly','sigmoid'],\n",
    "               'class_weight': ['balanced',None]\n",
    "            }\n",
    "\n",
    "# Create a base model\n",
    "svm = SVC(random_state = 0)\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = svm, param_grid = param_grid, \n",
    "                          cv = 5, n_jobs = -1, verbose = 2, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train_tfidf, train['topic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_grid = grid_search.best_estimator_\n",
    "grid_accuracy = evaluate(best_grid, X_test_tfidf, test['topic'])\n",
    "grid_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Improvement of {:0.2f}%.'.format( 100 * (grid_accuracy - base_accuracy) / base_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Multinominal Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "multiNB_pipeline = Pipeline([\n",
    "     ('vect', CountVectorizer()),\n",
    "     ('tfidf', TfidfTransformer()),\n",
    "     ('clf', MultinomialNB(alpha=1.0, fit_prior=True)), #GaussianNB\n",
    " ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiNB_model = multiNB_pipeline.fit(train['text'], train['topic'])\n",
    "multiNB_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = multiNB_model.score(test['text'], test['topic'])\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to file in the current working directory\n",
    "joblib_file = \"Models/Classification/multiNB_model_\"+ dt + \"_score_\"+ str(round(sc*100,2))+\".pkl\"\n",
    "joblib.dump(multiNB_model, joblib_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "predicted_test = multiNB_model.predict(test['text'])\n",
    "print(metrics.classification_report(test['topic'], predicted_test,target_names=target_names_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text = ['hedhi pedi wawekwe karantini',\n",
    "            'kupata maji safi shida',\n",
    "            'maambukizo kutoka kwa mama',\n",
    "            'udumavu wa elimu inayotolewa izingatie', \n",
    "            'Kituo afya kinatoa huduma mbaya wajawazito']\n",
    "new_text_cleaned = clean_for_prediction(new_text)\n",
    "predicted = svm_model.predict(new_text_cleaned)\n",
    "print(predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-05-09T13:37:32.971000Z",
     "iopub.status.busy": "2020-05-09T13:37:32.971000Z",
     "iopub.status.idle": "2020-05-09T13:37:32.974494Z",
     "shell.execute_reply": "2020-05-09T13:37:32.973501Z",
     "shell.execute_reply.started": "2020-05-09T13:37:32.971000Z"
    }
   },
   "source": [
    "# 5. Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "decisionTree_pipeline = Pipeline([\n",
    "     ('vect', CountVectorizer()),\n",
    "     ('tfidf', TfidfTransformer()),\n",
    "     ('clf', tree.DecisionTreeClassifier()),\n",
    " ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decisionTree_model = decisionTree_pipeline.fit(train['text'], train['topic'])\n",
    "decisionTree_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = decisionTree_model.score(test['text'], test['topic'])\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to file in the current working directory\n",
    "joblib_file = \"Models/decisionTree_model.pkl\"\n",
    "joblib.dump(decisionTree_model, joblib_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "predicted_test = decisionTree_model.predict(test['text'])\n",
    "print(metrics.classification_report(test['topic'], predicted_test,target_names=target_names_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text = ['hedhi pedi', \n",
    "            'hatua kujikinga homa mapafu kunawa mikono',\n",
    "            'udumavu wa mtoto','elimu inayotolewa izingatie', \n",
    "            'Kituo afya kinatoa huduma mbaya wajawazito',\n",
    "            'Magonjwa yatokanayo na ngono isiyo salama na maambukizo mengine yanaweza pia kuongeza uwezekano wa maambukizo']\n",
    "new_text_cleaned = clean_for_prediction(new_text)\n",
    "predicted = svm_model.predict(new_text_cleaned)\n",
    "print(predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Forests of randomized trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "randomForest_pipeline = Pipeline([\n",
    "     ('vect', CountVectorizer()),\n",
    "     ('tfidf', TfidfTransformer()),\n",
    "     ('clf', RandomForestClassifier(random_state = 0, n_jobs=-1, criterion= 'gini',\n",
    " max_depth= None,\n",
    " max_features= 'auto',\n",
    " min_samples_leaf= 1,\n",
    " min_samples_split= 5,\n",
    " n_estimators= 1000\n",
    "                                   )),\n",
    " ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomForest_model = randomForest_pipeline.fit(train['text'], train['topic'])\n",
    "randomForest_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = randomForest_model.score(test['text'], test['topic'])\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to file in the current working directory\n",
    "joblib_file = \"Models/RandomForest_model.pkl.z\"\n",
    "joblib.dump(randomForest_model, joblib_file, compress=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "predicted_test = randomForest_model.predict(test['text'])\n",
    "print(metrics.classification_report(test['topic'], predicted_test,target_names=target_names_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text = ['hedhi pedi', \n",
    "            'hatua kujikinga homa mapafu kunawa mikono',\n",
    "            'udumavu wa mtoto','elimu inayotolewa izingatie', \n",
    "            'Kituo afya kinatoa huduma mbaya wajawazito',\n",
    "            'Magonjwa yatokanayo na ngono isiyo salama na maambukizo mengine yanaweza pia kuongeza uwezekano wa maambukizo']\n",
    "new_text_cleaned = clean_for_prediction(new_text)\n",
    "predicted = randomForest_model.predict(new_text_cleaned)\n",
    "print(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Search with Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.arange(0, 1500, 50)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt', 'log2']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "criterion= ['gini', 'entropy']\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap,\n",
    "              'criterion':criterion}\n",
    "\n",
    "pprint(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(random_state = 0, n_jobs=-1)\n",
    "rf_random = RandomizedSearchCV(estimator=rf, param_distributions=random_grid,\n",
    "                              n_iter = 100, scoring='neg_mean_absolute_error', \n",
    "                              cv = 5, verbose=2, random_state=0, n_jobs=-1,\n",
    "                              return_train_score=True)\n",
    "\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_train_tfidf, train['topic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Function\n",
    "def evaluate(model, test_features, test_labels):\n",
    "    \n",
    "    return model.score(test_features, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the Default Model\n",
    "base_model = RandomForestClassifier()\n",
    "base_model.fit(X_train_tfidf, train['topic'])\n",
    "base_accuracy = evaluate(base_model, X_test_tfidf, test['topic'])\n",
    "base_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the Best Random Search Model\n",
    "best_random = rf_random.best_estimator_\n",
    "random_accuracy = evaluate(best_random, X_test_tfidf, test['topic'])\n",
    "random_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Improvement of {:0.2f}%.'.format( 100 * (random_accuracy - base_accuracy) / base_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create the parameter grid based on the results of random search \n",
    "# {'criterion': 'gini',\n",
    "#  'max_depth': None,\n",
    "#  'max_features': 'auto',\n",
    "#  'min_samples_leaf': 1,\n",
    "#  'min_samples_split': 5,\n",
    "#  'n_estimators': 1000}\n",
    "\n",
    "param_grid = {    \n",
    "     'n_estimators':[985, 990, 1000],\n",
    "     'min_samples_split': [2,4, 5, 6],\n",
    "     'min_samples_leaf': [1],\n",
    "     'max_features': ['auto'],\n",
    "     'max_depth': [None],\n",
    "     'criterion': ['gini'] # 'entropy'\n",
    "}\n",
    "\n",
    "# Create a base model\n",
    "rf = RandomForestClassifier(random_state = 0, n_jobs=-1)\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "                          cv = 5, n_jobs = -1, verbose = 2, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train_tfidf, train['topic']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the Best Model from Grid Search\n",
    "best_grid = grid_search.best_estimator_\n",
    "grid_accuracy = evaluate(best_grid, X_test_tfidf, test['topic'])\n",
    "grid_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Improvement of {:0.2f}%.'.format( 100 * (grid_accuracy - base_accuracy) / base_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Nearest Neighbors Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = len(target_names_list)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "best_k = k+7\n",
    "knn_pipeline = Pipeline([\n",
    "     ('vect', CountVectorizer()),\n",
    "     ('tfidf', TfidfTransformer()),\n",
    "     ('clf', KNeighborsClassifier(best_k, weights='distance')),\n",
    " ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model = knn_pipeline.fit(train['text'], train['topic'])\n",
    "knn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = knn_model.score(test['text'], test['topic'])\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to file in the current working directory\n",
    "joblib_file = \"Models/Classification/knn_model_\"+ dt + \"_score_\"+ str(round(sc*100,2))+\".pkl\"\n",
    "joblib.dump(knn_model, joblib_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "predicted_test = knn_model.predict(test['text'])\n",
    "print(metrics.classification_report(test['topic'], predicted_test,target_names=target_names_list))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
